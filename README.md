# Flexible CNN Training Framework

Train and tune modern convolutional neural networks (CNN) with **one command and one YAML file**. This repository wraps a clean PyTorch training loop, multiple reference architectures, a reproducible hyperâ€‘parameter search, a readyâ€‘toâ€‘use loader for the [MAMe fineâ€‘art dataset](https://mame-datasets.github.io/), and utilities for evaluation and oneâ€‘off scripts.

## âœ¨ Key features
- **Plugâ€‘andâ€‘play architectures** â€“ *StandardCNN*, *InceptionNet*, *InceptionNetV3* and *TransferLearningCNN* (VGGâ€‘19 backbone) selectable via `model.type`.
- **Singleâ€‘file configuration** â€“ every experiment described in a humanâ€‘readable YAML; no code changes needed.
- **Recursive grid search** â€“ coordinateâ€‘wise search that quickly narrows numeric, boolean **and** categorical hyperâ€‘parameters.
- **Mixedâ€‘precision & GPU ready** â€“ automatic casting and gradient scaling out of the box.
- **Evaluation & reporting** â€“ run `test.py` to compute accuracy, precision, recall, F1, confusion matrix, and export as text, CSV & LaTeX tables.
- **Extensible** â€“ drop your own model under `implementations/` and add one import in `utils/training_helpers.py`.

## âš’ Installation
```bash
# 1. create env (Python â‰¥3.9)
conda create -n cnn-env python=3.10
conda activate cnn-env

# 2. install deps
pip install -r requirements.txt
```

> **Note**  
> GPU training requires a CUDAâ€‘enabled version of **PyTorch**.

## ğŸš€ Quick start

### 1. Prepare the dataset
Download **MAMe** (â‰Œ12â€¯GB) and unpack it so it looks like:
```
mame-dataset/
â”œâ”€â”€ data/                 # images
â”‚   â””â”€â”€ 00000.jpg
â”œâ”€â”€ MAMe_dataset.csv      # metadata & splits
```
The loader looks for this directory in the current working directory. For a different dataset, provide a CSV with `filename,class` columns and update `utils/dataset.py`.

### 2. Train a model
```bash
time python train.py --config configs/standardSearch.yaml
```
This will:
1. Build the model from the YAML  
2. Train for `training.epochs` epochs  
3. Save the best checkpoint as `best_<model>.pth`

### 3. Hyperâ€‘parameter search (optional)
```bash
time python train.py --config configs/standardSearch.yaml --tune
```
Rangeâ€‘division grid search recurses over every entry in `tuning.param_grid` until convergence. Results are logged to CSV (`tuning.search_csv`) and the best set is written back into the live config object before full training.

### 4. Evaluate a trained model
```bash
python test.py --config configs/standardSearch.yaml --weights best_StandardCNN.pth --output results/StandardCNN
```
Generates:
- `metrics.txt`: overall & per-class metrics  
- `confusion_matrix.csv`  
- LaTeX tables: `metrics.tex` & `confusion_matrix.tex`

## ğŸ”§ Utility scripts
Additional one-off utilities live in the `scripts/` folder. For example:
```
scripts/
â”œâ”€â”€ preprocess.py           # custom data transforms
â”œâ”€â”€ download_weights.py     # download pretrained backbones
â””â”€â”€ ...
```
Modify or extend as needed.

## ğŸ”§ Configuration reference
Each section of `config.yaml` is described in code comments and Pydantic docstrings (`utils/config.py`). See `configs/standardSearch.yaml` for a full example.

## ğŸ“ Repository layout
```
.
â”œâ”€â”€ train.py                   # Training entry point
â”œâ”€â”€ test.py                    # Evaluation script
â”œâ”€â”€ implementations/           # Model definitions
â”‚   â”œâ”€â”€ classifier.py
â”‚   â”œâ”€â”€ standard.py
â”‚   â”œâ”€â”€ inception.py
â”‚   â”œâ”€â”€ inceptionv3.py
â”‚   â””â”€â”€ transfer_learning.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ training_helpers.py
â”‚   â””â”€â”€ hyperparameter_search.py
â”œâ”€â”€ scripts/                   # Utility and one-off scripts
â”‚   â””â”€â”€ *.py
â”œâ”€â”€ configs/                   # YAML experiment configs
â”‚   â””â”€â”€ *.yaml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“œ License
MIT Â© 2025 Antonio Loboâ€‘Santos & Alejandro GuzmÃ¡nâ€‘Requena

