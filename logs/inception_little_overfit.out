Model: InceptionNet(
  (stem): Stem(
    (stem): Sequential(
      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
  )
  (stages): Sequential(
    (0): InceptionStage(
      (stage): Sequential(
        (0): InceptionBlock(
          (b0): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (b1): Sequential(
            (0): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(48, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b2): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(64, 96, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(96, 96, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
            (4): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 96, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(96, 96, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b3): Sequential(
            (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
            (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU(inplace=True)
          )
          (se): SE(
            (avg): AdaptiveAvgPool2d(output_size=1)
            (fc): Sequential(
              (0): Linear(in_features=256, out_features=16, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=256, bias=False)
              (3): Sigmoid()
            )
          )
          (dropout): Identity()
        )
        (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      )
    )
    (1): InceptionStage(
      (stage): Sequential(
        (0): InceptionBlock(
          (b0): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (b1): Sequential(
            (0): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b2): Sequential(
            (0): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
            (4): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b3): Sequential(
            (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
            (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU(inplace=True)
          )
          (se): SE(
            (avg): AdaptiveAvgPool2d(output_size=1)
            (fc): Sequential(
              (0): Linear(in_features=448, out_features=28, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=28, out_features=448, bias=False)
              (3): Sigmoid()
            )
          )
          (dropout): Identity()
        )
        (1): InceptionBlock(
          (b0): Sequential(
            (0): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (b1): Sequential(
            (0): Conv2d(448, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b2): Sequential(
            (0): Conv2d(448, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
            (4): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b3): Sequential(
            (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
            (1): Conv2d(448, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU(inplace=True)
          )
          (se): SE(
            (avg): AdaptiveAvgPool2d(output_size=1)
            (fc): Sequential(
              (0): Linear(in_features=448, out_features=28, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=28, out_features=448, bias=False)
              (3): Sigmoid()
            )
          )
          (dropout): Identity()
        )
        (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      )
    )
    (2): InceptionStage(
      (stage): Sequential(
        (0): InceptionBlock(
          (b0): Sequential(
            (0): Conv2d(448, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (b1): Sequential(
            (0): Conv2d(448, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 160, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(160, 160, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b2): Sequential(
            (0): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(128, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
            (4): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b3): Sequential(
            (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
            (1): Conv2d(448, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU(inplace=True)
          )
          (se): SE(
            (avg): AdaptiveAvgPool2d(output_size=1)
            (fc): Sequential(
              (0): Linear(in_features=640, out_features=40, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=40, out_features=640, bias=False)
              (3): Sigmoid()
            )
          )
          (dropout): Identity()
        )
        (1): InceptionBlock(
          (b0): Sequential(
            (0): Conv2d(640, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (b1): Sequential(
            (0): Conv2d(640, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 160, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(160, 160, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b2): Sequential(
            (0): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(128, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
            (4): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b3): Sequential(
            (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
            (1): Conv2d(640, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU(inplace=True)
          )
          (se): SE(
            (avg): AdaptiveAvgPool2d(output_size=1)
            (fc): Sequential(
              (0): Linear(in_features=640, out_features=40, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=40, out_features=640, bias=False)
              (3): Sigmoid()
            )
          )
          (dropout): Identity()
        )
      )
    )
  )
  (pool): AdaptiveAvgPool2d(output_size=1)
  (classifier): Classifier(
    (net): Sequential(
      (0): Linear(in_features=640, out_features=2048, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.6, inplace=False)
      (3): Linear(in_features=2048, out_features=1024, bias=True)
      (4): ReLU(inplace=True)
      (5): Dropout(p=0.6, inplace=False)
      (6): Linear(in_features=1024, out_features=29, bias=True)
    )
  )
)
Overwriting model state from search_InceptionNet_mame.pth
Epoch 001/300: train_loss=2.6159 acc=0.198 | val_loss=2.2271 acc=0.304
Epoch 002/300: train_loss=2.0340 acc=0.362 | val_loss=1.9187 acc=0.410
Epoch 003/300: train_loss=1.8093 acc=0.429 | val_loss=1.6895 acc=0.468
Epoch 004/300: train_loss=1.6510 acc=0.476 | val_loss=1.4974 acc=0.517
Epoch 005/300: train_loss=1.5313 acc=0.510 | val_loss=1.4591 acc=0.529
Epoch 006/300: train_loss=1.4488 acc=0.540 | val_loss=1.3342 acc=0.579
Epoch 007/300: train_loss=1.3836 acc=0.559 | val_loss=1.2095 acc=0.599
Epoch 008/300: train_loss=1.3155 acc=0.584 | val_loss=1.1685 acc=0.603
Epoch 009/300: train_loss=1.2566 acc=0.598 | val_loss=1.1532 acc=0.620
Epoch 010/300: train_loss=1.2134 acc=0.610 | val_loss=1.1052 acc=0.643
Epoch 011/300: train_loss=1.1604 acc=0.629 | val_loss=1.1371 acc=0.632
Epoch 012/300: train_loss=1.1267 acc=0.640 | val_loss=1.0691 acc=0.654
Epoch 013/300: train_loss=1.0850 acc=0.654 | val_loss=1.1855 acc=0.619
Epoch 014/300: train_loss=1.0544 acc=0.666 | val_loss=1.0587 acc=0.671
Epoch 015/300: train_loss=1.0183 acc=0.676 | val_loss=0.9701 acc=0.685
Epoch 016/300: train_loss=0.9799 acc=0.688 | val_loss=1.0433 acc=0.657
Epoch 017/300: train_loss=0.9586 acc=0.697 | val_loss=1.0840 acc=0.656
Epoch 018/300: train_loss=0.9322 acc=0.704 | val_loss=0.9892 acc=0.683
Epoch 019/300: train_loss=0.9029 acc=0.714 | val_loss=0.9320 acc=0.703
Epoch 020/300: train_loss=0.8791 acc=0.722 | val_loss=0.9378 acc=0.697
Epoch 021/300: train_loss=0.8518 acc=0.728 | val_loss=0.9573 acc=0.702
Epoch 022/300: train_loss=0.8335 acc=0.735 | val_loss=0.9075 acc=0.723
Epoch 023/300: train_loss=0.8197 acc=0.741 | val_loss=0.9151 acc=0.713
Epoch 024/300: train_loss=0.7930 acc=0.748 | val_loss=0.8901 acc=0.710
Epoch 025/300: train_loss=0.7705 acc=0.755 | val_loss=0.8572 acc=0.728
Epoch 026/300: train_loss=0.7681 acc=0.757 | val_loss=0.9119 acc=0.722
Epoch 027/300: train_loss=0.7473 acc=0.765 | val_loss=0.8197 acc=0.735
Epoch 028/300: train_loss=0.7056 acc=0.772 | val_loss=0.9114 acc=0.727
Epoch 029/300: train_loss=0.7086 acc=0.776 | val_loss=0.8885 acc=0.746
Epoch 030/300: train_loss=0.7041 acc=0.773 | val_loss=0.8851 acc=0.726
Epoch 031/300: train_loss=0.6782 acc=0.782 | val_loss=0.8518 acc=0.727
Epoch 032/300: train_loss=0.6737 acc=0.787 | val_loss=0.7809 acc=0.752
Epoch 033/300: train_loss=0.6448 acc=0.793 | val_loss=0.8434 acc=0.741
Epoch 034/300: train_loss=0.6503 acc=0.792 | val_loss=0.8445 acc=0.742
Epoch 035/300: train_loss=0.6291 acc=0.801 | val_loss=0.8753 acc=0.725
Epoch 036/300: train_loss=0.6100 acc=0.807 | val_loss=0.8715 acc=0.734
Epoch 037/300: train_loss=0.6053 acc=0.809 | val_loss=0.9254 acc=0.723
Epoch 038/300: train_loss=0.6002 acc=0.808 | val_loss=0.8442 acc=0.754
Epoch 039/300: train_loss=0.5689 acc=0.817 | val_loss=0.8668 acc=0.744
Epoch 040/300: train_loss=0.5729 acc=0.818 | val_loss=0.7760 acc=0.759
Epoch 041/300: train_loss=0.5659 acc=0.818 | val_loss=0.8316 acc=0.753
Epoch 042/300: train_loss=0.5531 acc=0.822 | val_loss=0.8638 acc=0.752
Epoch 043/300: train_loss=0.5308 acc=0.829 | val_loss=0.8357 acc=0.750
Epoch 044/300: train_loss=0.5508 acc=0.827 | val_loss=0.8373 acc=0.759
Epoch 045/300: train_loss=0.5327 acc=0.830 | val_loss=0.8878 acc=0.749
Epoch 046/300: train_loss=0.5038 acc=0.838 | val_loss=1.0067 acc=0.720
Epoch 047/300: train_loss=0.5170 acc=0.835 | val_loss=0.9439 acc=0.748
Epoch 048/300: train_loss=0.5012 acc=0.840 | val_loss=0.8648 acc=0.738
Epoch 049/300: train_loss=0.5040 acc=0.840 | val_loss=0.8504 acc=0.743
Epoch 050/300: train_loss=0.4909 acc=0.843 | val_loss=0.8894 acc=0.757
Epoch 051/300: train_loss=0.4679 acc=0.849 | val_loss=0.9143 acc=0.731
Epoch 052/300: train_loss=0.4639 acc=0.850 | val_loss=0.7984 acc=0.774
Epoch 053/300: train_loss=0.4713 acc=0.851 | val_loss=1.0381 acc=0.730
Epoch 054/300: train_loss=0.4718 acc=0.850 | val_loss=0.8500 acc=0.750
Epoch 055/300: train_loss=0.4539 acc=0.858 | val_loss=0.8011 acc=0.773
Epoch 056/300: train_loss=0.4352 acc=0.862 | val_loss=0.9085 acc=0.758
Epoch 057/300: train_loss=0.4500 acc=0.858 | val_loss=0.9023 acc=0.756
Epoch 058/300: train_loss=0.4277 acc=0.865 | val_loss=0.9606 acc=0.720
Epoch 059/300: train_loss=0.4270 acc=0.866 | val_loss=0.8833 acc=0.774
Epoch 060/300: train_loss=0.4044 acc=0.868 | val_loss=0.8961 acc=0.766
Epoch 061/300: train_loss=0.3925 acc=0.875 | val_loss=1.0511 acc=0.738
Epoch 062/300: train_loss=0.4175 acc=0.868 | val_loss=0.9153 acc=0.759
Epoch 063/300: train_loss=0.3975 acc=0.874 | val_loss=0.9072 acc=0.770
Epoch 064/300: train_loss=0.3960 acc=0.875 | val_loss=0.9043 acc=0.761
Epoch 065/300: train_loss=0.3885 acc=0.876 | val_loss=0.9355 acc=0.766
Epoch 066/300: train_loss=0.3941 acc=0.877 | val_loss=0.8346 acc=0.776
Epoch 067/300: train_loss=0.3758 acc=0.878 | val_loss=0.9097 acc=0.763
Epoch 068/300: train_loss=0.3661 acc=0.885 | val_loss=0.8770 acc=0.769
Epoch 069/300: train_loss=0.3742 acc=0.885 | val_loss=0.8617 acc=0.781
Epoch 070/300: train_loss=0.3627 acc=0.887 | val_loss=0.8151 acc=0.786
Epoch 071/300: train_loss=0.3350 acc=0.893 | val_loss=0.9453 acc=0.762
Epoch 072/300: train_loss=0.3651 acc=0.882 | val_loss=0.8938 acc=0.763
Epoch 073/300: train_loss=0.3439 acc=0.892 | val_loss=1.0664 acc=0.751
Epoch 074/300: train_loss=0.3428 acc=0.893 | val_loss=0.8717 acc=0.774
Epoch 075/300: train_loss=0.3502 acc=0.888 | val_loss=0.9252 acc=0.763
Epoch 076/300: train_loss=0.3265 acc=0.897 | val_loss=0.9821 acc=0.753
Epoch 077/300: train_loss=0.3234 acc=0.899 | val_loss=1.0270 acc=0.737
Epoch 078/300: train_loss=0.3129 acc=0.899 | val_loss=0.9293 acc=0.783
Epoch 079/300: train_loss=0.3219 acc=0.898 | val_loss=0.9542 acc=0.765
Epoch 080/300: train_loss=0.3202 acc=0.900 | val_loss=0.8834 acc=0.769
Epoch 081/300: train_loss=0.3079 acc=0.903 | val_loss=0.8702 acc=0.774
Epoch 082/300: train_loss=0.3028 acc=0.905 | val_loss=0.8918 acc=0.770
Epoch 083/300: train_loss=0.2922 acc=0.907 | val_loss=0.9888 acc=0.774
Epoch 084/300: train_loss=0.2983 acc=0.906 | val_loss=1.1047 acc=0.746
Epoch 085/300: train_loss=0.3244 acc=0.900 | val_loss=0.9730 acc=0.772
Epoch 086/300: train_loss=0.3041 acc=0.906 | val_loss=0.9559 acc=0.769
Epoch 087/300: train_loss=0.2800 acc=0.911 | val_loss=1.0388 acc=0.734
Epoch 088/300: train_loss=0.3038 acc=0.908 | val_loss=1.0355 acc=0.756
Epoch 089/300: train_loss=0.2870 acc=0.913 | val_loss=0.9183 acc=0.768
Epoch 090/300: train_loss=0.2704 acc=0.915 | val_loss=0.9303 acc=0.778
Early stopping triggered after 90 epochs with no improvement over 20.
Loading best model for final evaluation...
Test loss: 0.7923 | Test acc: 0.784
