Model: StandardCNN(
  (features): Sequential(
    (0): ConvBlock(
      (block): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      )
    )
    (1): ConvBlock(
      (block): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      )
    )
    (2): ConvBlock(
      (block): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      )
    )
  )
  (pool): Sequential(
    (0): AdaptiveAvgPool2d(output_size=1)
    (1): Flatten(start_dim=1, end_dim=-1)
  )
  (classifier): Classifier(
    (net): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.193, inplace=False)
      (3): Linear(in_features=256, out_features=29, bias=True)
    )
  )
)
Overwriting model state from searchS_StandardCNN_mame.pth
Patience for early stopping: 50 epochs
Training for 300 epochs
Epoch 001/300: train_loss=2.4389 acc=0.251 | val_loss=2.1408 acc=0.325
Epoch 002/300: train_loss=2.0811 acc=0.352 | val_loss=1.9310 acc=0.399
Epoch 003/300: train_loss=1.8717 acc=0.427 | val_loss=1.8170 acc=0.448
Epoch 004/300: train_loss=1.7339 acc=0.460 | val_loss=1.6870 acc=0.494
Epoch 005/300: train_loss=1.6293 acc=0.491 | val_loss=1.7253 acc=0.469
Epoch 006/300: train_loss=1.5677 acc=0.506 | val_loss=1.7206 acc=0.470
Epoch 007/300: train_loss=1.4896 acc=0.530 | val_loss=1.4983 acc=0.544
Epoch 008/300: train_loss=1.4277 acc=0.550 | val_loss=1.2987 acc=0.594
Epoch 009/300: train_loss=1.3709 acc=0.568 | val_loss=1.2617 acc=0.597
Epoch 010/300: train_loss=1.3262 acc=0.576 | val_loss=1.4102 acc=0.577
Epoch 011/300: train_loss=1.2892 acc=0.590 | val_loss=1.2580 acc=0.606
Epoch 012/300: train_loss=1.2564 acc=0.598 | val_loss=1.2174 acc=0.614
Epoch 013/300: train_loss=1.2230 acc=0.607 | val_loss=1.2453 acc=0.596
Epoch 014/300: train_loss=1.1815 acc=0.620 | val_loss=1.1696 acc=0.625
Epoch 015/300: train_loss=1.1620 acc=0.627 | val_loss=1.2334 acc=0.624
Epoch 016/300: train_loss=1.1371 acc=0.638 | val_loss=1.6958 acc=0.513
Epoch 017/300: train_loss=1.1186 acc=0.637 | val_loss=1.3958 acc=0.573
Epoch 018/300: train_loss=1.0883 acc=0.648 | val_loss=1.1431 acc=0.631
Epoch 019/300: train_loss=1.0604 acc=0.655 | val_loss=1.1640 acc=0.632
Epoch 020/300: train_loss=1.0474 acc=0.661 | val_loss=1.1020 acc=0.641
Epoch 021/300: train_loss=1.0252 acc=0.665 | val_loss=1.1399 acc=0.645
Epoch 022/300: train_loss=1.0045 acc=0.677 | val_loss=1.0427 acc=0.668
Epoch 023/300: train_loss=0.9882 acc=0.677 | val_loss=1.0043 acc=0.684
Epoch 024/300: train_loss=0.9648 acc=0.687 | val_loss=1.0141 acc=0.688
Epoch 025/300: train_loss=0.9492 acc=0.693 | val_loss=0.9871 acc=0.693
Epoch 026/300: train_loss=0.9361 acc=0.693 | val_loss=1.0059 acc=0.683
Epoch 027/300: train_loss=0.9181 acc=0.699 | val_loss=1.0406 acc=0.674
Epoch 028/300: train_loss=0.8954 acc=0.706 | val_loss=1.0529 acc=0.677
Epoch 029/300: train_loss=0.8896 acc=0.709 | val_loss=1.1216 acc=0.665
Epoch 030/300: train_loss=0.8738 acc=0.709 | val_loss=1.0634 acc=0.668
Epoch 031/300: train_loss=0.8618 acc=0.717 | val_loss=1.1533 acc=0.670
Epoch 032/300: train_loss=0.8442 acc=0.723 | val_loss=1.0851 acc=0.674
Epoch 033/300: train_loss=0.8304 acc=0.729 | val_loss=0.9888 acc=0.697
Epoch 034/300: train_loss=0.8138 acc=0.730 | val_loss=1.0691 acc=0.680
Epoch 035/300: train_loss=0.8130 acc=0.733 | val_loss=0.9410 acc=0.711
Epoch 036/300: train_loss=0.7883 acc=0.739 | val_loss=0.9825 acc=0.714
Epoch 037/300: train_loss=0.7784 acc=0.742 | val_loss=0.9527 acc=0.723
Epoch 038/300: train_loss=0.7654 acc=0.746 | val_loss=0.9459 acc=0.723
Epoch 039/300: train_loss=0.7699 acc=0.743 | val_loss=1.0589 acc=0.688
Epoch 040/300: train_loss=0.7377 acc=0.754 | val_loss=0.9979 acc=0.709
Epoch 041/300: train_loss=0.7337 acc=0.756 | val_loss=0.9223 acc=0.717
Epoch 042/300: train_loss=0.7328 acc=0.757 | val_loss=1.0288 acc=0.706
Epoch 043/300: train_loss=0.7256 acc=0.758 | val_loss=1.0284 acc=0.701
Epoch 044/300: train_loss=0.7100 acc=0.760 | val_loss=1.0159 acc=0.694
Epoch 045/300: train_loss=0.6947 acc=0.766 | val_loss=0.9723 acc=0.714
Epoch 046/300: train_loss=0.6858 acc=0.770 | val_loss=1.1090 acc=0.663
Epoch 047/300: train_loss=0.6804 acc=0.773 | val_loss=1.0016 acc=0.703
Epoch 048/300: train_loss=0.6727 acc=0.775 | val_loss=0.9513 acc=0.725
Epoch 049/300: train_loss=0.6581 acc=0.778 | val_loss=0.9799 acc=0.735
Epoch 050/300: train_loss=0.6554 acc=0.782 | val_loss=0.9470 acc=0.738
Epoch 051/300: train_loss=0.6485 acc=0.782 | val_loss=0.9496 acc=0.727
Epoch 052/300: train_loss=0.6310 acc=0.786 | val_loss=1.2230 acc=0.674
Epoch 053/300: train_loss=0.6303 acc=0.784 | val_loss=0.9370 acc=0.737
Epoch 054/300: train_loss=0.6230 acc=0.786 | val_loss=0.9431 acc=0.739
Epoch 055/300: train_loss=0.6137 acc=0.790 | val_loss=0.8958 acc=0.739
Epoch 056/300: train_loss=0.6044 acc=0.798 | val_loss=1.0142 acc=0.726
Epoch 057/300: train_loss=0.5956 acc=0.797 | val_loss=0.9648 acc=0.738
Epoch 058/300: train_loss=0.5921 acc=0.800 | val_loss=0.9221 acc=0.754
Epoch 059/300: train_loss=0.5675 acc=0.807 | val_loss=1.1570 acc=0.676
Epoch 060/300: train_loss=0.5705 acc=0.803 | val_loss=0.9321 acc=0.741
Epoch 061/300: train_loss=0.5648 acc=0.807 | val_loss=0.9853 acc=0.734
Epoch 062/300: train_loss=0.5598 acc=0.807 | val_loss=0.9196 acc=0.750
Epoch 063/300: train_loss=0.5523 acc=0.812 | val_loss=0.9418 acc=0.741
Epoch 064/300: train_loss=0.5453 acc=0.812 | val_loss=0.9800 acc=0.738
Epoch 065/300: train_loss=0.5459 acc=0.814 | val_loss=0.9943 acc=0.738
Epoch 066/300: train_loss=0.5321 acc=0.817 | val_loss=1.0094 acc=0.732
Epoch 067/300: train_loss=0.5392 acc=0.814 | val_loss=1.0762 acc=0.706
Epoch 068/300: train_loss=0.5111 acc=0.824 | val_loss=0.9029 acc=0.759
Epoch 069/300: train_loss=0.5101 acc=0.824 | val_loss=0.9563 acc=0.745
Epoch 070/300: train_loss=0.5035 acc=0.828 | val_loss=0.9214 acc=0.756
Epoch 071/300: train_loss=0.4983 acc=0.830 | val_loss=1.0280 acc=0.738
Epoch 072/300: train_loss=0.4936 acc=0.831 | val_loss=1.0455 acc=0.723
Epoch 073/300: train_loss=0.4778 acc=0.835 | val_loss=0.9689 acc=0.730
Epoch 074/300: train_loss=0.4839 acc=0.829 | val_loss=0.9848 acc=0.754
Epoch 075/300: train_loss=0.4833 acc=0.834 | val_loss=1.0448 acc=0.742
Epoch 076/300: train_loss=0.4595 acc=0.843 | val_loss=0.9961 acc=0.747
Epoch 077/300: train_loss=0.4599 acc=0.839 | val_loss=1.0023 acc=0.745
Epoch 078/300: train_loss=0.4588 acc=0.842 | val_loss=1.1372 acc=0.712
Epoch 079/300: train_loss=0.4619 acc=0.840 | val_loss=1.0383 acc=0.741
Epoch 080/300: train_loss=0.4527 acc=0.842 | val_loss=0.9629 acc=0.748
Epoch 081/300: train_loss=0.4377 acc=0.847 | val_loss=1.0127 acc=0.734
Epoch 082/300: train_loss=0.4326 acc=0.846 | val_loss=0.9735 acc=0.743
Epoch 083/300: train_loss=0.4308 acc=0.849 | val_loss=1.0185 acc=0.750
Epoch 084/300: train_loss=0.4319 acc=0.851 | val_loss=1.0072 acc=0.746
Epoch 085/300: train_loss=0.4236 acc=0.852 | val_loss=0.9193 acc=0.763
Epoch 086/300: train_loss=0.4135 acc=0.855 | val_loss=1.0095 acc=0.762
Epoch 087/300: train_loss=0.4138 acc=0.856 | val_loss=1.0144 acc=0.751
Epoch 088/300: train_loss=0.4038 acc=0.858 | val_loss=1.1673 acc=0.714
Epoch 089/300: train_loss=0.4006 acc=0.860 | val_loss=1.0365 acc=0.748
Epoch 090/300: train_loss=0.3985 acc=0.862 | val_loss=1.0780 acc=0.745
Epoch 091/300: train_loss=0.4065 acc=0.857 | val_loss=1.0861 acc=0.725
Epoch 092/300: train_loss=0.3946 acc=0.863 | val_loss=1.0710 acc=0.748
Epoch 093/300: train_loss=0.3852 acc=0.867 | val_loss=0.9836 acc=0.760
Epoch 094/300: train_loss=0.3794 acc=0.868 | val_loss=1.0610 acc=0.753
Epoch 095/300: train_loss=0.3798 acc=0.869 | val_loss=0.9840 acc=0.756
Epoch 096/300: train_loss=0.3641 acc=0.873 | val_loss=1.0890 acc=0.753
Epoch 097/300: train_loss=0.3601 acc=0.873 | val_loss=1.1748 acc=0.730
Epoch 098/300: train_loss=0.3596 acc=0.874 | val_loss=1.0339 acc=0.752
Epoch 099/300: train_loss=0.3588 acc=0.875 | val_loss=1.0058 acc=0.774
Epoch 100/300: train_loss=0.3469 acc=0.879 | val_loss=1.1459 acc=0.732
Epoch 101/300: train_loss=0.3468 acc=0.880 | val_loss=1.0416 acc=0.746
Epoch 102/300: train_loss=0.3343 acc=0.883 | val_loss=1.0865 acc=0.746
Epoch 103/300: train_loss=0.3374 acc=0.880 | val_loss=1.0619 acc=0.757
Epoch 104/300: train_loss=0.3347 acc=0.879 | val_loss=1.1661 acc=0.752
Epoch 105/300: train_loss=0.3267 acc=0.886 | val_loss=1.0590 acc=0.766
Epoch 106/300: train_loss=0.3291 acc=0.884 | val_loss=1.2423 acc=0.742
Epoch 107/300: train_loss=0.3281 acc=0.887 | val_loss=1.0906 acc=0.763
Epoch 108/300: train_loss=0.3123 acc=0.890 | val_loss=1.1151 acc=0.754
Epoch 109/300: train_loss=0.3160 acc=0.890 | val_loss=1.0646 acc=0.755
Epoch 110/300: train_loss=0.3165 acc=0.889 | val_loss=1.0204 acc=0.761
Epoch 111/300: train_loss=0.3035 acc=0.892 | val_loss=1.0780 acc=0.746
Epoch 112/300: train_loss=0.3038 acc=0.893 | val_loss=1.1584 acc=0.763
Epoch 113/300: train_loss=0.3028 acc=0.896 | val_loss=1.1354 acc=0.769
Epoch 114/300: train_loss=0.2967 acc=0.895 | val_loss=1.0606 acc=0.765
Epoch 115/300: train_loss=0.3018 acc=0.895 | val_loss=1.0869 acc=0.764
Epoch 116/300: train_loss=0.2872 acc=0.899 | val_loss=1.0703 acc=0.766
Epoch 117/300: train_loss=0.2918 acc=0.898 | val_loss=1.1280 acc=0.770
Epoch 118/300: train_loss=0.2855 acc=0.902 | val_loss=1.1169 acc=0.757
Epoch 119/300: train_loss=0.2713 acc=0.904 | val_loss=1.1937 acc=0.752
Epoch 120/300: train_loss=0.2801 acc=0.901 | val_loss=1.3268 acc=0.730
Epoch 121/300: train_loss=0.2774 acc=0.903 | val_loss=1.1366 acc=0.745
Epoch 122/300: train_loss=0.2742 acc=0.903 | val_loss=1.1123 acc=0.761
Epoch 123/300: train_loss=0.2648 acc=0.908 | val_loss=1.0999 acc=0.764
Epoch 124/300: train_loss=0.2584 acc=0.911 | val_loss=1.1234 acc=0.757
Epoch 125/300: train_loss=0.2608 acc=0.907 | val_loss=1.2213 acc=0.751
Epoch 126/300: train_loss=0.2451 acc=0.912 | val_loss=1.2244 acc=0.756
Epoch 127/300: train_loss=0.2523 acc=0.911 | val_loss=1.1762 acc=0.770
Epoch 128/300: train_loss=0.2444 acc=0.914 | val_loss=1.1442 acc=0.756
Epoch 129/300: train_loss=0.2509 acc=0.912 | val_loss=1.0944 acc=0.772
Epoch 130/300: train_loss=0.2488 acc=0.914 | val_loss=1.0414 acc=0.782
Epoch 131/300: train_loss=0.2332 acc=0.917 | val_loss=1.1385 acc=0.761
Epoch 132/300: train_loss=0.2346 acc=0.916 | val_loss=1.1724 acc=0.762
Epoch 133/300: train_loss=0.2382 acc=0.918 | val_loss=1.1724 acc=0.756
Epoch 134/300: train_loss=0.2333 acc=0.918 | val_loss=1.1191 acc=0.774
Epoch 135/300: train_loss=0.2401 acc=0.915 | val_loss=1.1441 acc=0.779
Epoch 136/300: train_loss=0.2244 acc=0.922 | val_loss=1.1404 acc=0.758
Epoch 137/300: train_loss=0.2370 acc=0.917 | val_loss=1.1799 acc=0.766
Epoch 138/300: train_loss=0.2216 acc=0.922 | val_loss=1.1438 acc=0.779
Epoch 139/300: train_loss=0.2305 acc=0.919 | val_loss=1.2253 acc=0.770
Epoch 140/300: train_loss=0.2164 acc=0.924 | val_loss=1.1659 acc=0.764
Epoch 141/300: train_loss=0.2066 acc=0.927 | val_loss=1.2468 acc=0.769
Epoch 142/300: train_loss=0.2072 acc=0.928 | val_loss=1.1164 acc=0.776
Epoch 143/300: train_loss=0.2096 acc=0.927 | val_loss=1.2366 acc=0.762
Epoch 144/300: train_loss=0.1990 acc=0.931 | val_loss=1.2442 acc=0.776
Epoch 145/300: train_loss=0.1949 acc=0.928 | val_loss=1.2601 acc=0.769
Epoch 146/300: train_loss=0.2024 acc=0.927 | val_loss=1.2388 acc=0.760
Epoch 147/300: train_loss=0.2045 acc=0.930 | val_loss=1.2371 acc=0.761
Epoch 148/300: train_loss=0.1961 acc=0.931 | val_loss=1.2543 acc=0.758
Epoch 149/300: train_loss=0.1895 acc=0.931 | val_loss=1.1997 acc=0.767
Epoch 150/300: train_loss=0.1930 acc=0.931 | val_loss=1.2466 acc=0.777
Epoch 151/300: train_loss=0.1899 acc=0.933 | val_loss=1.1417 acc=0.768
Epoch 152/300: train_loss=0.1832 acc=0.936 | val_loss=1.1806 acc=0.774
Epoch 153/300: train_loss=0.1775 acc=0.938 | val_loss=1.2366 acc=0.774
Epoch 154/300: train_loss=0.1799 acc=0.937 | val_loss=1.1647 acc=0.767
Epoch 155/300: train_loss=0.1817 acc=0.937 | val_loss=1.2413 acc=0.770
Epoch 156/300: train_loss=0.1722 acc=0.938 | val_loss=1.1976 acc=0.768
Epoch 157/300: train_loss=0.1694 acc=0.940 | val_loss=1.1933 acc=0.771
Epoch 158/300: train_loss=0.1689 acc=0.942 | val_loss=1.2372 acc=0.776
Epoch 159/300: train_loss=0.1650 acc=0.944 | val_loss=1.2391 acc=0.766
Epoch 160/300: train_loss=0.1737 acc=0.937 | val_loss=1.2560 acc=0.770
Epoch 161/300: train_loss=0.1666 acc=0.941 | val_loss=1.2828 acc=0.761
Epoch 162/300: train_loss=0.1672 acc=0.940 | val_loss=1.2893 acc=0.760
Epoch 163/300: train_loss=0.1583 acc=0.944 | val_loss=1.2174 acc=0.774
Epoch 164/300: train_loss=0.1559 acc=0.945 | val_loss=1.2384 acc=0.777
Epoch 165/300: train_loss=0.1565 acc=0.945 | val_loss=1.2236 acc=0.777
Epoch 166/300: train_loss=0.1504 acc=0.947 | val_loss=1.2482 acc=0.773
Epoch 167/300: train_loss=0.1545 acc=0.946 | val_loss=1.2896 acc=0.772
Epoch 168/300: train_loss=0.1667 acc=0.942 | val_loss=1.2721 acc=0.769
Epoch 169/300: train_loss=0.1561 acc=0.945 | val_loss=1.2095 acc=0.770
Epoch 170/300: train_loss=0.1396 acc=0.952 | val_loss=1.3218 acc=0.772
Epoch 171/300: train_loss=0.1447 acc=0.950 | val_loss=1.2390 acc=0.771
Epoch 172/300: train_loss=0.1369 acc=0.952 | val_loss=1.2345 acc=0.771
Epoch 173/300: train_loss=0.1510 acc=0.948 | val_loss=1.2248 acc=0.783
Epoch 174/300: train_loss=0.1296 acc=0.956 | val_loss=1.2913 acc=0.774
Epoch 175/300: train_loss=0.1336 acc=0.952 | val_loss=1.2557 acc=0.784
Epoch 176/300: train_loss=0.1266 acc=0.955 | val_loss=1.2892 acc=0.776
Epoch 177/300: train_loss=0.1389 acc=0.952 | val_loss=1.3329 acc=0.777
Epoch 178/300: train_loss=0.1333 acc=0.953 | val_loss=1.3584 acc=0.779
Epoch 179/300: train_loss=0.1316 acc=0.953 | val_loss=1.3479 acc=0.772
Epoch 180/300: train_loss=0.1321 acc=0.953 | val_loss=1.2243 acc=0.786
Epoch 181/300: train_loss=0.1248 acc=0.959 | val_loss=1.2999 acc=0.782
Epoch 182/300: train_loss=0.1219 acc=0.959 | val_loss=1.3041 acc=0.777
Epoch 183/300: train_loss=0.1191 acc=0.960 | val_loss=1.2961 acc=0.770
Epoch 184/300: train_loss=0.1229 acc=0.957 | val_loss=1.3259 acc=0.769
Epoch 185/300: train_loss=0.1218 acc=0.958 | val_loss=1.3323 acc=0.775
Epoch 186/300: train_loss=0.1312 acc=0.954 | val_loss=1.3374 acc=0.779
Epoch 187/300: train_loss=0.1183 acc=0.960 | val_loss=1.2611 acc=0.777
Epoch 188/300: train_loss=0.1191 acc=0.959 | val_loss=1.2938 acc=0.783
Epoch 189/300: train_loss=0.1160 acc=0.961 | val_loss=1.3830 acc=0.773
Epoch 190/300: train_loss=0.1154 acc=0.960 | val_loss=1.2943 acc=0.781
Epoch 191/300: train_loss=0.1103 acc=0.962 | val_loss=1.2757 acc=0.781
Epoch 192/300: train_loss=0.1130 acc=0.961 | val_loss=1.3544 acc=0.783
Epoch 193/300: train_loss=0.1159 acc=0.960 | val_loss=1.3568 acc=0.779
Epoch 194/300: train_loss=0.1005 acc=0.966 | val_loss=1.2969 acc=0.780
Epoch 195/300: train_loss=0.1074 acc=0.964 | val_loss=1.3088 acc=0.781
Epoch 196/300: train_loss=0.1035 acc=0.964 | val_loss=1.3083 acc=0.782
Epoch 197/300: train_loss=0.1054 acc=0.962 | val_loss=1.3404 acc=0.771
Epoch 198/300: train_loss=0.1016 acc=0.965 | val_loss=1.3615 acc=0.781
Epoch 199/300: train_loss=0.0947 acc=0.968 | val_loss=1.4110 acc=0.785
Epoch 200/300: train_loss=0.0960 acc=0.969 | val_loss=1.4019 acc=0.772
Epoch 201/300: train_loss=0.0946 acc=0.968 | val_loss=1.3446 acc=0.777
Epoch 202/300: train_loss=0.0955 acc=0.969 | val_loss=1.4037 acc=0.774
Epoch 203/300: train_loss=0.0958 acc=0.967 | val_loss=1.3359 acc=0.779
Epoch 204/300: train_loss=0.0919 acc=0.969 | val_loss=1.3624 acc=0.777
Epoch 205/300: train_loss=0.0992 acc=0.967 | val_loss=1.3097 acc=0.786
Epoch 206/300: train_loss=0.0940 acc=0.968 | val_loss=1.3716 acc=0.780
Epoch 207/300: train_loss=0.0895 acc=0.969 | val_loss=1.4360 acc=0.779
Epoch 208/300: train_loss=0.0881 acc=0.970 | val_loss=1.3776 acc=0.781
Epoch 209/300: train_loss=0.0867 acc=0.971 | val_loss=1.3800 acc=0.790
Epoch 210/300: train_loss=0.0859 acc=0.972 | val_loss=1.4434 acc=0.777
Epoch 211/300: train_loss=0.0881 acc=0.970 | val_loss=1.3634 acc=0.788
Epoch 212/300: train_loss=0.0880 acc=0.971 | val_loss=1.4666 acc=0.772
Epoch 213/300: train_loss=0.0836 acc=0.972 | val_loss=1.3747 acc=0.779
Epoch 214/300: train_loss=0.0819 acc=0.972 | val_loss=1.4380 acc=0.781
Epoch 215/300: train_loss=0.0834 acc=0.971 | val_loss=1.3966 acc=0.788
Epoch 216/300: train_loss=0.0834 acc=0.973 | val_loss=1.4832 acc=0.773
Epoch 217/300: train_loss=0.0799 acc=0.974 | val_loss=1.4797 acc=0.776
Epoch 218/300: train_loss=0.0801 acc=0.972 | val_loss=1.4200 acc=0.774
Epoch 219/300: train_loss=0.0788 acc=0.975 | val_loss=1.4410 acc=0.786
Epoch 220/300: train_loss=0.0777 acc=0.974 | val_loss=1.4563 acc=0.777
Epoch 221/300: train_loss=0.0723 acc=0.976 | val_loss=1.4333 acc=0.777
Epoch 222/300: train_loss=0.0718 acc=0.977 | val_loss=1.3706 acc=0.784
Epoch 223/300: train_loss=0.0741 acc=0.976 | val_loss=1.4299 acc=0.786
Epoch 224/300: train_loss=0.0740 acc=0.976 | val_loss=1.4510 acc=0.787
Epoch 225/300: train_loss=0.0713 acc=0.977 | val_loss=1.4869 acc=0.781
Epoch 226/300: train_loss=0.0722 acc=0.975 | val_loss=1.4643 acc=0.786
Epoch 227/300: train_loss=0.0704 acc=0.976 | val_loss=1.4628 acc=0.781
Epoch 228/300: train_loss=0.0741 acc=0.977 | val_loss=1.3974 acc=0.787
Epoch 229/300: train_loss=0.0697 acc=0.976 | val_loss=1.4385 acc=0.777
Epoch 230/300: train_loss=0.0707 acc=0.975 | val_loss=1.4453 acc=0.782
Epoch 231/300: train_loss=0.0670 acc=0.979 | val_loss=1.4465 acc=0.781
Epoch 232/300: train_loss=0.0659 acc=0.978 | val_loss=1.4611 acc=0.786
Epoch 233/300: train_loss=0.0691 acc=0.976 | val_loss=1.4161 acc=0.782
Epoch 234/300: train_loss=0.0666 acc=0.977 | val_loss=1.4028 acc=0.779
Epoch 235/300: train_loss=0.0679 acc=0.977 | val_loss=1.4712 acc=0.782
Epoch 236/300: train_loss=0.0621 acc=0.980 | val_loss=1.4587 acc=0.780
Epoch 237/300: train_loss=0.0635 acc=0.979 | val_loss=1.4931 acc=0.781
Epoch 238/300: train_loss=0.0590 acc=0.981 | val_loss=1.4978 acc=0.779
Epoch 239/300: train_loss=0.0586 acc=0.981 | val_loss=1.4479 acc=0.785
Epoch 240/300: train_loss=0.0611 acc=0.980 | val_loss=1.4627 acc=0.786
Epoch 241/300: train_loss=0.0583 acc=0.980 | val_loss=1.4443 acc=0.788
Epoch 242/300: train_loss=0.0586 acc=0.981 | val_loss=1.4192 acc=0.785
Epoch 243/300: train_loss=0.0596 acc=0.981 | val_loss=1.4660 acc=0.790
Epoch 244/300: train_loss=0.0571 acc=0.981 | val_loss=1.4670 acc=0.787
Epoch 245/300: train_loss=0.0591 acc=0.981 | val_loss=1.4783 acc=0.786
Epoch 246/300: train_loss=0.0576 acc=0.982 | val_loss=1.4787 acc=0.786
Epoch 247/300: train_loss=0.0574 acc=0.982 | val_loss=1.4944 acc=0.777
Epoch 248/300: train_loss=0.0617 acc=0.979 | val_loss=1.5047 acc=0.783
Epoch 249/300: train_loss=0.0582 acc=0.981 | val_loss=1.5354 acc=0.791
Epoch 250/300: train_loss=0.0570 acc=0.982 | val_loss=1.4893 acc=0.777
Epoch 251/300: train_loss=0.0566 acc=0.982 | val_loss=1.4778 acc=0.784
Epoch 252/300: train_loss=0.0554 acc=0.981 | val_loss=1.4643 acc=0.785
Epoch 253/300: train_loss=0.0523 acc=0.984 | val_loss=1.4903 acc=0.790
Epoch 254/300: train_loss=0.0509 acc=0.985 | val_loss=1.5052 acc=0.785
Epoch 255/300: train_loss=0.0570 acc=0.980 | val_loss=1.4511 acc=0.786
Epoch 256/300: train_loss=0.0560 acc=0.982 | val_loss=1.4720 acc=0.789
Epoch 257/300: train_loss=0.0527 acc=0.983 | val_loss=1.4644 acc=0.786
Epoch 258/300: train_loss=0.0511 acc=0.984 | val_loss=1.4803 acc=0.791
Epoch 259/300: train_loss=0.0555 acc=0.982 | val_loss=1.5280 acc=0.790
Epoch 260/300: train_loss=0.0479 acc=0.985 | val_loss=1.4884 acc=0.785
Epoch 261/300: train_loss=0.0511 acc=0.984 | val_loss=1.4661 acc=0.788
Epoch 262/300: train_loss=0.0522 acc=0.983 | val_loss=1.4845 acc=0.774
Epoch 263/300: train_loss=0.0504 acc=0.984 | val_loss=1.4981 acc=0.782
Epoch 264/300: train_loss=0.0514 acc=0.983 | val_loss=1.5026 acc=0.788
Epoch 265/300: train_loss=0.0480 acc=0.984 | val_loss=1.4941 acc=0.790
Epoch 266/300: train_loss=0.0529 acc=0.981 | val_loss=1.4903 acc=0.786
Epoch 267/300: train_loss=0.0483 acc=0.985 | val_loss=1.4958 acc=0.783
Epoch 268/300: train_loss=0.0513 acc=0.982 | val_loss=1.4899 acc=0.786
Epoch 269/300: train_loss=0.0502 acc=0.983 | val_loss=1.4784 acc=0.788
Epoch 270/300: train_loss=0.0509 acc=0.985 | val_loss=1.4958 acc=0.787
Epoch 271/300: train_loss=0.0463 acc=0.986 | val_loss=1.4892 acc=0.788
Epoch 272/300: train_loss=0.0496 acc=0.985 | val_loss=1.4949 acc=0.788
Epoch 273/300: train_loss=0.0483 acc=0.985 | val_loss=1.4947 acc=0.789
Epoch 274/300: train_loss=0.0507 acc=0.984 | val_loss=1.4657 acc=0.788
Epoch 275/300: train_loss=0.0506 acc=0.984 | val_loss=1.5005 acc=0.783
Epoch 276/300: train_loss=0.0476 acc=0.984 | val_loss=1.4775 acc=0.786
Epoch 277/300: train_loss=0.0474 acc=0.986 | val_loss=1.4973 acc=0.787
Epoch 278/300: train_loss=0.0465 acc=0.985 | val_loss=1.4907 acc=0.783
Epoch 279/300: train_loss=0.0458 acc=0.986 | val_loss=1.4950 acc=0.793
Epoch 280/300: train_loss=0.0481 acc=0.985 | val_loss=1.5094 acc=0.787
Epoch 281/300: train_loss=0.0474 acc=0.986 | val_loss=1.4851 acc=0.789
Epoch 282/300: train_loss=0.0479 acc=0.985 | val_loss=1.4845 acc=0.784
Epoch 283/300: train_loss=0.0458 acc=0.985 | val_loss=1.5096 acc=0.786
Epoch 284/300: train_loss=0.0472 acc=0.986 | val_loss=1.4756 acc=0.786
Epoch 285/300: train_loss=0.0454 acc=0.985 | val_loss=1.4922 acc=0.789
Epoch 286/300: train_loss=0.0496 acc=0.985 | val_loss=1.4790 acc=0.785
Epoch 287/300: train_loss=0.0456 acc=0.985 | val_loss=1.5074 acc=0.788
Epoch 288/300: train_loss=0.0422 acc=0.987 | val_loss=1.4825 acc=0.787
Epoch 289/300: train_loss=0.0495 acc=0.984 | val_loss=1.4877 acc=0.784
Epoch 290/300: train_loss=0.0466 acc=0.985 | val_loss=1.4947 acc=0.789
Epoch 291/300: train_loss=0.0467 acc=0.985 | val_loss=1.4848 acc=0.785
Epoch 292/300: train_loss=0.0469 acc=0.985 | val_loss=1.4856 acc=0.786
Epoch 293/300: train_loss=0.0463 acc=0.984 | val_loss=1.4847 acc=0.792
Epoch 294/300: train_loss=0.0464 acc=0.985 | val_loss=1.4969 acc=0.786
Epoch 295/300: train_loss=0.0471 acc=0.986 | val_loss=1.5190 acc=0.788
Epoch 296/300: train_loss=0.0475 acc=0.984 | val_loss=1.4972 acc=0.780
Epoch 297/300: train_loss=0.0466 acc=0.986 | val_loss=1.4806 acc=0.788
Epoch 298/300: train_loss=0.0442 acc=0.986 | val_loss=1.4806 acc=0.783
Epoch 299/300: train_loss=0.0490 acc=0.984 | val_loss=1.4870 acc=0.786
Epoch 300/300: train_loss=0.0458 acc=0.985 | val_loss=1.5070 acc=0.786
Loading best model for final evaluation...
Test loss: 1.4567 | Test acc: 0.791
