Model: InceptionNet(
  (stem): Stem(
    (stem): Sequential(
      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
  )
  (stages): Sequential(
    (0): InceptionStage(
      (stage): Sequential(
        (0): InceptionBlock(
          (b0): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (b1): Sequential(
            (0): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(48, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b2): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(64, 96, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(96, 96, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
            (4): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 96, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(96, 96, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b3): Sequential(
            (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
            (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU(inplace=True)
          )
          (se): SE(
            (avg): AdaptiveAvgPool2d(output_size=1)
            (fc): Sequential(
              (0): Linear(in_features=256, out_features=16, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=256, bias=False)
              (3): Sigmoid()
            )
          )
          (dropout): Identity()
        )
        (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      )
    )
    (1): InceptionStage(
      (stage): Sequential(
        (0): InceptionBlock(
          (b0): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (b1): Sequential(
            (0): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b2): Sequential(
            (0): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
            (4): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b3): Sequential(
            (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
            (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU(inplace=True)
          )
          (se): SE(
            (avg): AdaptiveAvgPool2d(output_size=1)
            (fc): Sequential(
              (0): Linear(in_features=448, out_features=28, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=28, out_features=448, bias=False)
              (3): Sigmoid()
            )
          )
          (dropout): Identity()
        )
        (1): InceptionBlock(
          (b0): Sequential(
            (0): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (b1): Sequential(
            (0): Conv2d(448, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b2): Sequential(
            (0): Conv2d(448, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
            (4): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b3): Sequential(
            (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
            (1): Conv2d(448, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU(inplace=True)
          )
          (se): SE(
            (avg): AdaptiveAvgPool2d(output_size=1)
            (fc): Sequential(
              (0): Linear(in_features=448, out_features=28, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=28, out_features=448, bias=False)
              (3): Sigmoid()
            )
          )
          (dropout): Identity()
        )
        (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      )
    )
    (2): InceptionStage(
      (stage): Sequential(
        (0): InceptionBlock(
          (b0): Sequential(
            (0): Conv2d(448, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (b1): Sequential(
            (0): Conv2d(448, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 160, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(160, 160, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b2): Sequential(
            (0): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(128, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
            (4): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b3): Sequential(
            (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
            (1): Conv2d(448, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU(inplace=True)
          )
          (se): SE(
            (avg): AdaptiveAvgPool2d(output_size=1)
            (fc): Sequential(
              (0): Linear(in_features=640, out_features=40, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=40, out_features=640, bias=False)
              (3): Sigmoid()
            )
          )
          (dropout): Identity()
        )
        (1): InceptionBlock(
          (b0): Sequential(
            (0): Conv2d(640, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (b1): Sequential(
            (0): Conv2d(640, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(96, 160, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(160, 160, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b2): Sequential(
            (0): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(128, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
            (4): FactorisedConv(
              (block): Sequential(
                (0): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
                (1): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
                (2): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
                (3): Sequential(
                  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (1): ReLU(inplace=True)
                )
              )
            )
          )
          (b3): Sequential(
            (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
            (1): Conv2d(640, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU(inplace=True)
          )
          (se): SE(
            (avg): AdaptiveAvgPool2d(output_size=1)
            (fc): Sequential(
              (0): Linear(in_features=640, out_features=40, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=40, out_features=640, bias=False)
              (3): Sigmoid()
            )
          )
          (dropout): Identity()
        )
      )
    )
  )
  (pool): AdaptiveAvgPool2d(output_size=1)
  (classifier): Classifier(
    (net): Sequential(
      (0): Linear(in_features=640, out_features=2048, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.6, inplace=False)
      (3): Linear(in_features=2048, out_features=1024, bias=True)
      (4): ReLU(inplace=True)
      (5): Dropout(p=0.6, inplace=False)
      (6): Linear(in_features=1024, out_features=29, bias=True)
    )
  )
)
Overwriting model state from search_InceptionNet_mame.pth
Patience for early stopping: 40 epochs
Training for 300 epochs
Epoch 001/300: train_loss=2.5649 acc=0.205 | val_loss=2.0995 acc=0.339
Epoch 002/300: train_loss=1.9999 acc=0.365 | val_loss=1.8646 acc=0.418
Epoch 003/300: train_loss=1.7616 acc=0.438 | val_loss=1.6737 acc=0.479
Epoch 004/300: train_loss=1.5855 acc=0.500 | val_loss=1.5499 acc=0.528
Epoch 005/300: train_loss=1.4687 acc=0.534 | val_loss=1.5570 acc=0.509
Epoch 006/300: train_loss=1.3858 acc=0.558 | val_loss=1.2666 acc=0.592
Epoch 007/300: train_loss=1.2970 acc=0.581 | val_loss=1.2807 acc=0.570
Epoch 008/300: train_loss=1.2229 acc=0.607 | val_loss=1.3363 acc=0.572
Epoch 009/300: train_loss=1.1864 acc=0.620 | val_loss=1.2311 acc=0.603
Epoch 010/300: train_loss=1.1338 acc=0.637 | val_loss=1.1929 acc=0.622
Epoch 011/300: train_loss=1.0812 acc=0.655 | val_loss=1.0722 acc=0.650
Epoch 012/300: train_loss=1.0357 acc=0.665 | val_loss=1.1272 acc=0.634
Epoch 013/300: train_loss=0.9967 acc=0.678 | val_loss=1.0315 acc=0.679
Epoch 014/300: train_loss=0.9547 acc=0.693 | val_loss=1.0483 acc=0.670
Epoch 015/300: train_loss=0.9194 acc=0.704 | val_loss=0.9468 acc=0.693
Epoch 016/300: train_loss=0.8762 acc=0.721 | val_loss=0.9785 acc=0.686
Epoch 017/300: train_loss=0.8643 acc=0.721 | val_loss=0.9473 acc=0.697
Epoch 018/300: train_loss=0.8314 acc=0.735 | val_loss=0.9309 acc=0.696
Epoch 019/300: train_loss=0.7950 acc=0.744 | val_loss=0.9941 acc=0.681
Epoch 020/300: train_loss=0.7664 acc=0.755 | val_loss=0.9178 acc=0.718
Epoch 021/300: train_loss=0.7667 acc=0.751 | val_loss=0.9281 acc=0.712
Epoch 022/300: train_loss=0.7048 acc=0.769 | val_loss=0.9504 acc=0.710
Epoch 023/300: train_loss=0.6940 acc=0.778 | val_loss=0.9695 acc=0.712
Epoch 024/300: train_loss=0.6707 acc=0.783 | val_loss=0.8651 acc=0.731
Epoch 025/300: train_loss=0.6489 acc=0.792 | val_loss=0.9060 acc=0.731
Epoch 026/300: train_loss=0.6344 acc=0.792 | val_loss=0.8484 acc=0.752
Epoch 027/300: train_loss=0.6124 acc=0.800 | val_loss=0.9963 acc=0.712
Epoch 028/300: train_loss=0.6024 acc=0.807 | val_loss=0.9197 acc=0.723
Epoch 029/300: train_loss=0.5843 acc=0.812 | val_loss=0.9046 acc=0.733
Epoch 030/300: train_loss=0.5675 acc=0.816 | val_loss=0.9057 acc=0.719
Epoch 031/300: train_loss=0.5557 acc=0.822 | val_loss=0.8443 acc=0.744
Epoch 032/300: train_loss=0.5390 acc=0.826 | val_loss=0.9168 acc=0.735
Epoch 033/300: train_loss=0.5122 acc=0.830 | val_loss=0.9200 acc=0.748
Epoch 034/300: train_loss=0.5014 acc=0.836 | val_loss=0.9258 acc=0.739
Epoch 035/300: train_loss=0.4729 acc=0.846 | val_loss=1.1415 acc=0.703
Epoch 036/300: train_loss=0.4760 acc=0.847 | val_loss=0.9252 acc=0.741
Epoch 037/300: train_loss=0.4554 acc=0.851 | val_loss=0.8700 acc=0.753
Epoch 038/300: train_loss=0.4546 acc=0.853 | val_loss=0.9138 acc=0.740
Epoch 039/300: train_loss=0.4475 acc=0.854 | val_loss=0.9362 acc=0.743
Epoch 040/300: train_loss=0.4160 acc=0.865 | val_loss=0.8530 acc=0.766
Epoch 041/300: train_loss=0.4128 acc=0.865 | val_loss=0.9194 acc=0.744
Epoch 042/300: train_loss=0.3865 acc=0.874 | val_loss=0.9151 acc=0.750
Epoch 043/300: train_loss=0.3989 acc=0.871 | val_loss=0.9281 acc=0.750
Epoch 044/300: train_loss=0.3835 acc=0.873 | val_loss=0.9927 acc=0.730
Epoch 045/300: train_loss=0.3693 acc=0.881 | val_loss=0.8881 acc=0.765
Epoch 046/300: train_loss=0.3473 acc=0.887 | val_loss=0.8646 acc=0.770
Epoch 047/300: train_loss=0.3547 acc=0.886 | val_loss=0.9165 acc=0.759
Epoch 048/300: train_loss=0.3380 acc=0.890 | val_loss=1.0072 acc=0.737
Epoch 049/300: train_loss=0.3316 acc=0.891 | val_loss=0.9455 acc=0.743
Epoch 050/300: train_loss=0.3099 acc=0.900 | val_loss=0.8713 acc=0.770
Epoch 051/300: train_loss=0.3048 acc=0.900 | val_loss=0.9693 acc=0.764
Epoch 052/300: train_loss=0.3155 acc=0.899 | val_loss=0.9686 acc=0.758
Epoch 053/300: train_loss=0.2883 acc=0.907 | val_loss=1.1529 acc=0.739
Epoch 054/300: train_loss=0.2854 acc=0.909 | val_loss=1.0094 acc=0.761
Epoch 055/300: train_loss=0.3021 acc=0.903 | val_loss=0.8890 acc=0.781
Epoch 056/300: train_loss=0.2709 acc=0.912 | val_loss=1.0810 acc=0.748
Epoch 057/300: train_loss=0.2941 acc=0.907 | val_loss=0.8960 acc=0.763
Epoch 058/300: train_loss=0.2519 acc=0.918 | val_loss=1.0218 acc=0.757
Epoch 059/300: train_loss=0.2530 acc=0.921 | val_loss=1.0870 acc=0.746
Epoch 060/300: train_loss=0.2553 acc=0.919 | val_loss=0.9771 acc=0.759
Epoch 061/300: train_loss=0.2446 acc=0.920 | val_loss=1.0236 acc=0.757
Epoch 062/300: train_loss=0.2325 acc=0.924 | val_loss=1.0956 acc=0.736
Epoch 063/300: train_loss=0.2224 acc=0.926 | val_loss=0.9542 acc=0.762
Epoch 064/300: train_loss=0.2385 acc=0.926 | val_loss=1.0495 acc=0.768
Epoch 065/300: train_loss=0.2139 acc=0.930 | val_loss=0.9744 acc=0.788
Epoch 066/300: train_loss=0.2079 acc=0.933 | val_loss=1.0539 acc=0.761
Epoch 067/300: train_loss=0.2101 acc=0.932 | val_loss=1.1625 acc=0.760
Epoch 068/300: train_loss=0.2089 acc=0.934 | val_loss=1.1152 acc=0.757
Epoch 069/300: train_loss=0.1954 acc=0.938 | val_loss=1.0763 acc=0.763
Epoch 070/300: train_loss=0.2121 acc=0.934 | val_loss=1.0365 acc=0.741
Epoch 071/300: train_loss=0.2013 acc=0.938 | val_loss=0.9570 acc=0.769
Epoch 072/300: train_loss=0.1788 acc=0.944 | val_loss=1.1433 acc=0.756
Epoch 073/300: train_loss=0.1780 acc=0.944 | val_loss=1.0584 acc=0.768
Epoch 074/300: train_loss=0.1889 acc=0.942 | val_loss=1.1233 acc=0.767
Epoch 075/300: train_loss=0.1774 acc=0.943 | val_loss=1.0854 acc=0.767
Epoch 076/300: train_loss=0.1822 acc=0.943 | val_loss=1.1670 acc=0.758
Epoch 077/300: train_loss=0.1793 acc=0.944 | val_loss=1.0382 acc=0.781
Epoch 078/300: train_loss=0.1660 acc=0.949 | val_loss=1.0847 acc=0.782
Epoch 079/300: train_loss=0.1549 acc=0.952 | val_loss=1.0200 acc=0.783
Epoch 080/300: train_loss=0.1503 acc=0.955 | val_loss=1.0002 acc=0.786
Epoch 081/300: train_loss=0.1430 acc=0.955 | val_loss=1.0580 acc=0.781
Epoch 082/300: train_loss=0.1785 acc=0.946 | val_loss=1.0253 acc=0.782
Epoch 083/300: train_loss=0.1703 acc=0.947 | val_loss=0.9985 acc=0.781
Epoch 084/300: train_loss=0.1613 acc=0.951 | val_loss=1.0194 acc=0.786
Epoch 085/300: train_loss=0.1437 acc=0.955 | val_loss=1.0544 acc=0.773
Epoch 086/300: train_loss=0.1305 acc=0.958 | val_loss=1.2019 acc=0.761
Epoch 087/300: train_loss=0.1503 acc=0.954 | val_loss=1.1335 acc=0.770
Epoch 088/300: train_loss=0.1301 acc=0.958 | val_loss=1.2290 acc=0.766
Epoch 089/300: train_loss=0.1322 acc=0.959 | val_loss=1.1059 acc=0.777
Epoch 090/300: train_loss=0.1272 acc=0.961 | val_loss=1.2193 acc=0.768
Epoch 091/300: train_loss=0.1357 acc=0.959 | val_loss=1.0845 acc=0.790
Epoch 092/300: train_loss=0.1384 acc=0.957 | val_loss=1.1833 acc=0.759
Epoch 093/300: train_loss=0.1449 acc=0.955 | val_loss=1.1131 acc=0.782
Epoch 094/300: train_loss=0.1336 acc=0.959 | val_loss=1.0633 acc=0.781
Epoch 095/300: train_loss=0.1163 acc=0.964 | val_loss=1.3056 acc=0.748
Epoch 096/300: train_loss=0.1129 acc=0.965 | val_loss=1.2091 acc=0.772
Epoch 097/300: train_loss=0.1291 acc=0.960 | val_loss=1.2627 acc=0.749
Epoch 098/300: train_loss=0.1093 acc=0.967 | val_loss=1.3232 acc=0.765
Epoch 099/300: train_loss=0.1185 acc=0.964 | val_loss=1.0976 acc=0.795
Epoch 100/300: train_loss=0.1063 acc=0.968 | val_loss=1.1641 acc=0.782
Epoch 101/300: train_loss=0.1021 acc=0.968 | val_loss=1.2320 acc=0.766
Epoch 102/300: train_loss=0.1190 acc=0.963 | val_loss=1.1527 acc=0.772
Epoch 103/300: train_loss=0.1065 acc=0.968 | val_loss=1.1867 acc=0.773
Epoch 104/300: train_loss=0.0946 acc=0.971 | val_loss=1.3178 acc=0.766
Epoch 105/300: train_loss=0.1095 acc=0.967 | val_loss=1.1319 acc=0.778
Epoch 106/300: train_loss=0.1202 acc=0.966 | val_loss=1.2459 acc=0.768
Epoch 107/300: train_loss=0.0928 acc=0.974 | val_loss=1.1252 acc=0.772
Epoch 108/300: train_loss=0.0978 acc=0.970 | val_loss=1.1994 acc=0.770
Epoch 109/300: train_loss=0.0936 acc=0.971 | val_loss=1.1988 acc=0.777
Epoch 110/300: train_loss=0.0950 acc=0.970 | val_loss=1.2454 acc=0.777
Epoch 111/300: train_loss=0.1031 acc=0.969 | val_loss=1.2040 acc=0.775
Epoch 112/300: train_loss=0.1049 acc=0.968 | val_loss=1.1795 acc=0.786
Epoch 113/300: train_loss=0.0879 acc=0.974 | val_loss=1.1841 acc=0.798
Epoch 114/300: train_loss=0.0666 acc=0.979 | val_loss=1.2061 acc=0.788
Epoch 115/300: train_loss=0.0869 acc=0.973 | val_loss=1.2057 acc=0.786
Epoch 116/300: train_loss=0.1140 acc=0.966 | val_loss=1.1871 acc=0.773
Epoch 117/300: train_loss=0.0850 acc=0.975 | val_loss=1.2413 acc=0.770
Epoch 118/300: train_loss=0.0917 acc=0.973 | val_loss=1.1014 acc=0.796
Epoch 119/300: train_loss=0.0543 acc=0.984 | val_loss=1.1958 acc=0.798
Epoch 120/300: train_loss=0.0867 acc=0.974 | val_loss=1.2691 acc=0.741
Epoch 121/300: train_loss=0.0971 acc=0.970 | val_loss=1.1744 acc=0.787
Epoch 122/300: train_loss=0.0866 acc=0.974 | val_loss=1.2601 acc=0.768
Epoch 123/300: train_loss=0.0866 acc=0.973 | val_loss=1.0852 acc=0.793
Epoch 124/300: train_loss=0.0933 acc=0.972 | val_loss=1.2939 acc=0.771
Epoch 125/300: train_loss=0.0660 acc=0.980 | val_loss=1.2728 acc=0.781
Epoch 126/300: train_loss=0.0814 acc=0.975 | val_loss=1.1922 acc=0.768
Epoch 127/300: train_loss=0.0838 acc=0.975 | val_loss=1.2927 acc=0.776
Epoch 128/300: train_loss=0.0808 acc=0.976 | val_loss=1.2674 acc=0.777
Epoch 129/300: train_loss=0.0709 acc=0.979 | val_loss=1.2783 acc=0.786
Epoch 130/300: train_loss=0.0661 acc=0.980 | val_loss=1.2254 acc=0.788
Epoch 131/300: train_loss=0.0593 acc=0.982 | val_loss=1.2686 acc=0.795
Epoch 132/300: train_loss=0.0590 acc=0.982 | val_loss=1.3728 acc=0.767
Epoch 133/300: train_loss=0.0654 acc=0.982 | val_loss=1.3178 acc=0.778
Epoch 134/300: train_loss=0.0622 acc=0.981 | val_loss=1.3927 acc=0.781
Epoch 135/300: train_loss=0.0726 acc=0.978 | val_loss=1.3696 acc=0.781
Epoch 136/300: train_loss=0.0822 acc=0.976 | val_loss=1.4161 acc=0.761
Epoch 137/300: train_loss=0.0586 acc=0.983 | val_loss=1.3684 acc=0.771
Epoch 138/300: train_loss=0.0644 acc=0.982 | val_loss=1.3611 acc=0.780
Epoch 139/300: train_loss=0.0645 acc=0.980 | val_loss=1.2976 acc=0.777
Epoch 140/300: train_loss=0.0534 acc=0.984 | val_loss=1.4092 acc=0.777
Epoch 141/300: train_loss=0.0632 acc=0.981 | val_loss=1.2917 acc=0.784
Epoch 142/300: train_loss=0.0575 acc=0.984 | val_loss=1.2719 acc=0.797
Epoch 143/300: train_loss=0.0477 acc=0.987 | val_loss=1.5154 acc=0.768
Epoch 144/300: train_loss=0.0583 acc=0.983 | val_loss=1.3469 acc=0.772
Epoch 145/300: train_loss=0.0591 acc=0.982 | val_loss=1.3020 acc=0.762
Epoch 146/300: train_loss=0.0497 acc=0.985 | val_loss=1.4144 acc=0.781
Epoch 147/300: train_loss=0.0619 acc=0.982 | val_loss=1.2583 acc=0.792
Epoch 148/300: train_loss=0.0486 acc=0.985 | val_loss=1.3852 acc=0.788
Epoch 149/300: train_loss=0.0520 acc=0.985 | val_loss=1.3051 acc=0.781
Epoch 150/300: train_loss=0.0546 acc=0.985 | val_loss=1.2018 acc=0.797
Epoch 151/300: train_loss=0.0488 acc=0.986 | val_loss=1.3058 acc=0.790
Epoch 152/300: train_loss=0.0476 acc=0.986 | val_loss=1.3283 acc=0.790
Epoch 153/300: train_loss=0.0529 acc=0.985 | val_loss=1.3176 acc=0.783
Early stopping triggered after 153 epochs with no improvement over 40.
Loading best model for final evaluation...
Test loss: 1.1401 | Test acc: 0.784
