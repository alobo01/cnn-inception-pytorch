Model: TransferLearningCNN(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): ReLU(inplace=True)
    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): ReLU(inplace=True)
    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): ReLU(inplace=True)
    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (33): ReLU(inplace=True)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): ReLU(inplace=True)
    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (classifier): Classifier(
    (net): Sequential(
      (0): Linear(in_features=512, out_features=256, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=256, out_features=128, bias=True)
      (4): ReLU(inplace=True)
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=128, out_features=29, bias=True)
    )
  )
)
Overwriting model state from searchTL12_TransferLearningCNN_mame.pth
Patience for early stopping: 40 epochs
Training for 300 epochs
Epoch 001/300: train_loss=2.1682 acc=0.299 | val_loss=1.4359 acc=0.514
Epoch 002/300: train_loss=1.6619 acc=0.462 | val_loss=1.2753 acc=0.551
Epoch 003/300: train_loss=1.5442 acc=0.498 | val_loss=1.1921 acc=0.603
Epoch 004/300: train_loss=1.4507 acc=0.531 | val_loss=1.1226 acc=0.612
Epoch 005/300: train_loss=1.4133 acc=0.545 | val_loss=1.1152 acc=0.623
Epoch 006/300: train_loss=1.3526 acc=0.557 | val_loss=1.0706 acc=0.646
Epoch 007/300: train_loss=1.3396 acc=0.564 | val_loss=1.0953 acc=0.649
Epoch 008/300: train_loss=1.3023 acc=0.576 | val_loss=1.0634 acc=0.652
Epoch 009/300: train_loss=1.2952 acc=0.581 | val_loss=1.0421 acc=0.659
Epoch 010/300: train_loss=1.2765 acc=0.584 | val_loss=1.0380 acc=0.660
Epoch 011/300: train_loss=1.2416 acc=0.598 | val_loss=1.0343 acc=0.659
Epoch 012/300: train_loss=1.2362 acc=0.598 | val_loss=1.0240 acc=0.674
Epoch 013/300: train_loss=1.2326 acc=0.603 | val_loss=1.0265 acc=0.658
Epoch 014/300: train_loss=1.2064 acc=0.608 | val_loss=0.9953 acc=0.676
Epoch 015/300: train_loss=1.2034 acc=0.610 | val_loss=1.0609 acc=0.663
Epoch 016/300: train_loss=1.1957 acc=0.615 | val_loss=1.0112 acc=0.681
Epoch 017/300: train_loss=1.1889 acc=0.613 | val_loss=0.9857 acc=0.684
Epoch 018/300: train_loss=1.1910 acc=0.615 | val_loss=1.0090 acc=0.674
Epoch 019/300: train_loss=1.1808 acc=0.621 | val_loss=0.9889 acc=0.687
Epoch 020/300: train_loss=1.1606 acc=0.623 | val_loss=0.9994 acc=0.681
Epoch 021/300: train_loss=1.1539 acc=0.632 | val_loss=0.9973 acc=0.677
Epoch 022/300: train_loss=1.1718 acc=0.624 | val_loss=1.0080 acc=0.677
Epoch 023/300: train_loss=1.1607 acc=0.623 | val_loss=0.9792 acc=0.687
Epoch 024/300: train_loss=1.1508 acc=0.627 | val_loss=0.9927 acc=0.680
Epoch 025/300: train_loss=1.1326 acc=0.639 | val_loss=1.0075 acc=0.690
Epoch 026/300: train_loss=1.1371 acc=0.632 | val_loss=0.9901 acc=0.688
Epoch 027/300: train_loss=1.1365 acc=0.636 | val_loss=0.9936 acc=0.676
Epoch 028/300: train_loss=1.1329 acc=0.635 | val_loss=0.9921 acc=0.686
Epoch 029/300: train_loss=1.1238 acc=0.645 | val_loss=0.9889 acc=0.683
Epoch 030/300: train_loss=1.1168 acc=0.644 | val_loss=0.9983 acc=0.689
Epoch 031/300: train_loss=1.1198 acc=0.642 | val_loss=1.0098 acc=0.665
Epoch 032/300: train_loss=1.1216 acc=0.639 | val_loss=0.9906 acc=0.686
Epoch 033/300: train_loss=1.0988 acc=0.645 | val_loss=0.9765 acc=0.707
Epoch 034/300: train_loss=1.0946 acc=0.650 | val_loss=0.9490 acc=0.685
Epoch 035/300: train_loss=1.0838 acc=0.650 | val_loss=0.9970 acc=0.669
Epoch 036/300: train_loss=1.1263 acc=0.642 | val_loss=0.9630 acc=0.687
Epoch 037/300: train_loss=1.0903 acc=0.649 | val_loss=0.9694 acc=0.699
Epoch 038/300: train_loss=1.0771 acc=0.651 | val_loss=0.9599 acc=0.701
Epoch 039/300: train_loss=1.0875 acc=0.649 | val_loss=0.9752 acc=0.692
Epoch 040/300: train_loss=1.0911 acc=0.654 | val_loss=0.9885 acc=0.693
Epoch 041/300: train_loss=1.0779 acc=0.654 | val_loss=0.9526 acc=0.696
Epoch 042/300: train_loss=1.0770 acc=0.659 | val_loss=0.9710 acc=0.687
Epoch 043/300: train_loss=1.0692 acc=0.658 | val_loss=0.9386 acc=0.698
Epoch 044/300: train_loss=1.0655 acc=0.660 | val_loss=0.9543 acc=0.696
Epoch 045/300: train_loss=1.0642 acc=0.660 | val_loss=0.9441 acc=0.710
Epoch 046/300: train_loss=1.0629 acc=0.661 | val_loss=0.9772 acc=0.696
Epoch 047/300: train_loss=1.0680 acc=0.661 | val_loss=0.9772 acc=0.701
Epoch 048/300: train_loss=1.0765 acc=0.655 | val_loss=0.9555 acc=0.699
Epoch 049/300: train_loss=1.0656 acc=0.661 | val_loss=0.9727 acc=0.703
Epoch 050/300: train_loss=1.0571 acc=0.664 | val_loss=0.9770 acc=0.701
Epoch 051/300: train_loss=1.0277 acc=0.671 | val_loss=0.9552 acc=0.698
Epoch 052/300: train_loss=1.0566 acc=0.664 | val_loss=0.9886 acc=0.689
Epoch 053/300: train_loss=1.0479 acc=0.663 | val_loss=0.9542 acc=0.703
Epoch 054/300: train_loss=1.0429 acc=0.670 | val_loss=0.9722 acc=0.699
Epoch 055/300: train_loss=1.0348 acc=0.676 | val_loss=0.9712 acc=0.697
Epoch 056/300: train_loss=1.0358 acc=0.672 | val_loss=0.9686 acc=0.699
Epoch 057/300: train_loss=1.0252 acc=0.672 | val_loss=0.9777 acc=0.708
Epoch 058/300: train_loss=1.0436 acc=0.671 | val_loss=0.9664 acc=0.698
Epoch 059/300: train_loss=1.0344 acc=0.672 | val_loss=0.9598 acc=0.704
Epoch 060/300: train_loss=1.0164 acc=0.676 | val_loss=0.9627 acc=0.694
Epoch 061/300: train_loss=1.0336 acc=0.666 | val_loss=0.9452 acc=0.694
Epoch 062/300: train_loss=1.0292 acc=0.673 | val_loss=0.9735 acc=0.706
Epoch 063/300: train_loss=1.0154 acc=0.678 | val_loss=0.9723 acc=0.697
Epoch 064/300: train_loss=1.0309 acc=0.675 | val_loss=0.9757 acc=0.697
Epoch 065/300: train_loss=1.0345 acc=0.672 | val_loss=0.9409 acc=0.697
Epoch 066/300: train_loss=1.0184 acc=0.679 | val_loss=0.9699 acc=0.690
Epoch 067/300: train_loss=1.0175 acc=0.674 | val_loss=0.9649 acc=0.699
Epoch 068/300: train_loss=1.0098 acc=0.678 | val_loss=0.9682 acc=0.699
Epoch 069/300: train_loss=1.0001 acc=0.679 | val_loss=0.9438 acc=0.708
Epoch 070/300: train_loss=1.0066 acc=0.676 | val_loss=0.9427 acc=0.704
Epoch 071/300: train_loss=1.0058 acc=0.679 | val_loss=0.9439 acc=0.706
Epoch 072/300: train_loss=1.0056 acc=0.679 | val_loss=0.9526 acc=0.701
Epoch 073/300: train_loss=0.9818 acc=0.683 | val_loss=0.9729 acc=0.690
Epoch 074/300: train_loss=0.9890 acc=0.682 | val_loss=0.9556 acc=0.701
Epoch 075/300: train_loss=0.9792 acc=0.689 | val_loss=0.9834 acc=0.690
Epoch 076/300: train_loss=0.9999 acc=0.680 | val_loss=0.9584 acc=0.701
Epoch 077/300: train_loss=0.9783 acc=0.690 | val_loss=0.9697 acc=0.689
Epoch 078/300: train_loss=0.9702 acc=0.689 | val_loss=0.9541 acc=0.698
Epoch 079/300: train_loss=0.9803 acc=0.686 | val_loss=0.9452 acc=0.714
Epoch 080/300: train_loss=0.9857 acc=0.686 | val_loss=0.9648 acc=0.697
Epoch 081/300: train_loss=0.9743 acc=0.683 | val_loss=0.9610 acc=0.710
Epoch 082/300: train_loss=0.9580 acc=0.697 | val_loss=0.9513 acc=0.706
Epoch 083/300: train_loss=0.9797 acc=0.690 | val_loss=0.9892 acc=0.698
Epoch 084/300: train_loss=0.9772 acc=0.689 | val_loss=0.9431 acc=0.716
Epoch 085/300: train_loss=0.9661 acc=0.690 | val_loss=0.9460 acc=0.719
Epoch 086/300: train_loss=0.9614 acc=0.691 | val_loss=0.9417 acc=0.702
Epoch 087/300: train_loss=0.9624 acc=0.687 | val_loss=0.9615 acc=0.697
Epoch 088/300: train_loss=0.9597 acc=0.693 | val_loss=0.9568 acc=0.694
Epoch 089/300: train_loss=0.9591 acc=0.691 | val_loss=0.9354 acc=0.706
Epoch 090/300: train_loss=0.9602 acc=0.690 | val_loss=0.9568 acc=0.701
Epoch 091/300: train_loss=0.9596 acc=0.692 | val_loss=0.9588 acc=0.703
Epoch 092/300: train_loss=0.9393 acc=0.693 | val_loss=0.9465 acc=0.710
Epoch 093/300: train_loss=0.9583 acc=0.696 | val_loss=0.9560 acc=0.697
Epoch 094/300: train_loss=0.9435 acc=0.699 | val_loss=0.9334 acc=0.707
Epoch 095/300: train_loss=0.9448 acc=0.699 | val_loss=0.9528 acc=0.715
Epoch 096/300: train_loss=0.9401 acc=0.694 | val_loss=0.9672 acc=0.702
Epoch 097/300: train_loss=0.9304 acc=0.697 | val_loss=0.9631 acc=0.713
Epoch 098/300: train_loss=0.9334 acc=0.697 | val_loss=0.9599 acc=0.701
Epoch 099/300: train_loss=0.9401 acc=0.701 | val_loss=0.9682 acc=0.703
Epoch 100/300: train_loss=0.9381 acc=0.697 | val_loss=0.9872 acc=0.701
Epoch 101/300: train_loss=0.9288 acc=0.704 | val_loss=0.9703 acc=0.708
Epoch 102/300: train_loss=0.9188 acc=0.702 | val_loss=0.9714 acc=0.701
Epoch 103/300: train_loss=0.9105 acc=0.708 | val_loss=0.9394 acc=0.714
Epoch 104/300: train_loss=0.9197 acc=0.704 | val_loss=0.9515 acc=0.703
Epoch 105/300: train_loss=0.9123 acc=0.706 | val_loss=0.9508 acc=0.705
Epoch 106/300: train_loss=0.9111 acc=0.705 | val_loss=0.9131 acc=0.722
Epoch 107/300: train_loss=0.9194 acc=0.707 | val_loss=0.9489 acc=0.706
Epoch 108/300: train_loss=0.9200 acc=0.702 | val_loss=0.9616 acc=0.708
Epoch 109/300: train_loss=0.8920 acc=0.711 | val_loss=0.9606 acc=0.703
Epoch 110/300: train_loss=0.8888 acc=0.712 | val_loss=0.9635 acc=0.709
Epoch 111/300: train_loss=0.8871 acc=0.715 | val_loss=0.9388 acc=0.720
Epoch 112/300: train_loss=0.9193 acc=0.704 | val_loss=0.9388 acc=0.721
Epoch 113/300: train_loss=0.8878 acc=0.710 | val_loss=0.9441 acc=0.720
Epoch 114/300: train_loss=0.8870 acc=0.711 | val_loss=0.9314 acc=0.721
Epoch 115/300: train_loss=0.8881 acc=0.713 | val_loss=0.9278 acc=0.710
Epoch 116/300: train_loss=0.8792 acc=0.712 | val_loss=0.9430 acc=0.721
Epoch 117/300: train_loss=0.8724 acc=0.714 | val_loss=0.9346 acc=0.721
Epoch 118/300: train_loss=0.8821 acc=0.712 | val_loss=0.9522 acc=0.716
Epoch 119/300: train_loss=0.8813 acc=0.712 | val_loss=0.9306 acc=0.721
Epoch 120/300: train_loss=0.8859 acc=0.712 | val_loss=0.9398 acc=0.714
Epoch 121/300: train_loss=0.8795 acc=0.716 | val_loss=0.9141 acc=0.732
Epoch 122/300: train_loss=0.8720 acc=0.717 | val_loss=0.9901 acc=0.705
Epoch 123/300: train_loss=0.8472 acc=0.719 | val_loss=0.9296 acc=0.712
Epoch 124/300: train_loss=0.8493 acc=0.721 | val_loss=0.9447 acc=0.720
Epoch 125/300: train_loss=0.8588 acc=0.724 | val_loss=0.9559 acc=0.723
Epoch 126/300: train_loss=0.8652 acc=0.720 | val_loss=0.9166 acc=0.726
Epoch 127/300: train_loss=0.8629 acc=0.722 | val_loss=0.9535 acc=0.708
Epoch 128/300: train_loss=0.8477 acc=0.723 | val_loss=0.9366 acc=0.724
Epoch 129/300: train_loss=0.8355 acc=0.727 | val_loss=0.9439 acc=0.721
Epoch 130/300: train_loss=0.8358 acc=0.726 | val_loss=0.9531 acc=0.714
Epoch 131/300: train_loss=0.8388 acc=0.726 | val_loss=0.9583 acc=0.710
Epoch 132/300: train_loss=0.8445 acc=0.722 | val_loss=0.9504 acc=0.717
Epoch 133/300: train_loss=0.8416 acc=0.727 | val_loss=0.9240 acc=0.723
Epoch 134/300: train_loss=0.8427 acc=0.725 | val_loss=0.9368 acc=0.721
Epoch 135/300: train_loss=0.8314 acc=0.734 | val_loss=0.9460 acc=0.720
Epoch 136/300: train_loss=0.8320 acc=0.726 | val_loss=0.9353 acc=0.725
Epoch 137/300: train_loss=0.8312 acc=0.730 | val_loss=0.9209 acc=0.725
Epoch 138/300: train_loss=0.8287 acc=0.729 | val_loss=0.9213 acc=0.726
Epoch 139/300: train_loss=0.8114 acc=0.735 | val_loss=0.9517 acc=0.729
Epoch 140/300: train_loss=0.8284 acc=0.734 | val_loss=0.9747 acc=0.718
Epoch 141/300: train_loss=0.8245 acc=0.732 | val_loss=0.9870 acc=0.721
Epoch 142/300: train_loss=0.7968 acc=0.739 | val_loss=0.9479 acc=0.717
Epoch 143/300: train_loss=0.8072 acc=0.738 | val_loss=0.9428 acc=0.720
Epoch 144/300: train_loss=0.7934 acc=0.738 | val_loss=0.9500 acc=0.711
Epoch 145/300: train_loss=0.8064 acc=0.734 | val_loss=0.9413 acc=0.716
Epoch 146/300: train_loss=0.7835 acc=0.739 | val_loss=0.9479 acc=0.728
Epoch 147/300: train_loss=0.7871 acc=0.739 | val_loss=0.9655 acc=0.723
Epoch 148/300: train_loss=0.7987 acc=0.742 | val_loss=0.9607 acc=0.722
Epoch 149/300: train_loss=0.7740 acc=0.745 | val_loss=0.9505 acc=0.724
Epoch 150/300: train_loss=0.7950 acc=0.739 | val_loss=0.9678 acc=0.714
Epoch 151/300: train_loss=0.7822 acc=0.742 | val_loss=0.9442 acc=0.726
Epoch 152/300: train_loss=0.7656 acc=0.750 | val_loss=0.9517 acc=0.732
Epoch 153/300: train_loss=0.7733 acc=0.747 | val_loss=0.9597 acc=0.712
Epoch 154/300: train_loss=0.7634 acc=0.745 | val_loss=0.9560 acc=0.723
Epoch 155/300: train_loss=0.7646 acc=0.748 | val_loss=0.9413 acc=0.723
Epoch 156/300: train_loss=0.7721 acc=0.744 | val_loss=0.9354 acc=0.732
Epoch 157/300: train_loss=0.7693 acc=0.749 | val_loss=0.9439 acc=0.717
Epoch 158/300: train_loss=0.7512 acc=0.754 | val_loss=0.9515 acc=0.722
Epoch 159/300: train_loss=0.7585 acc=0.749 | val_loss=0.9256 acc=0.727
Epoch 160/300: train_loss=0.7626 acc=0.748 | val_loss=0.9544 acc=0.721
Epoch 161/300: train_loss=0.7410 acc=0.754 | val_loss=0.9651 acc=0.728
Early stopping triggered after 161 epochs with no improvement over 40.
Loading best model for final evaluation...
Test loss: 0.9400 | Test acc: 0.710
