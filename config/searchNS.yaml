# --- Dataset ------------------------------------------------------
dataset:
  name: MAMe
  img_size: 256        # Keeps native resolution; stem reduces it to 64×64

# --- Model --------------------------------------------------------
model:
  type: InceptionNet
  init_channels: 64    # Initial channels in the stem
  use_bn: true         # BatchNorm in all convolutions
  dropout_cls: 0.4     # Dropout before the final classifier

  # ---------------------------------------------------------------
  # Inception stages ≈ GoogLeNet extended with SE + residual
  # ---------------------------------------------------------------
  stage_cfgs:
    # Stage 0 ➜ 64×64
    - - { b0: 64,  b1: [48,  64],  b2: [64,  96],   pool_proj: 32,
          use_bn: true,  use_se: true,   residual: false }

    # Stage 1 ➜ 32×32 (2 modules)
    - - { b0: 128, b1: [96, 128], b2: [96, 128], pool_proj: 64,
          use_bn: true,  use_se: true,   residual: true }
      - { b0: 128, b1: [96, 128], b2: [96, 128], pool_proj: 64,
          use_bn: true,  use_se: true,   residual: true }

    # Stage 2 ➜ 16×16 (2 modules)
    - - { b0: 192, b1: [96, 160], b2: [128,192], pool_proj: 96,
          use_bn: true,  use_se: true,   residual: true }
      - { b0: 192, b1: [96, 160], b2: [128,192], pool_proj: 96,
          use_bn: true,  use_se: true,   residual: true }


augmentations:
  active: true
  resize: [256, 256]
  random_horizontal_flip: true
  random_rotation: 30
  random_resized_crop: [256, 256]
  scale: [0.8, 1.0]
  normalize:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

# --- Training -----------------------------------------------------
training:
  epochs: 300           # Long training for stabilization
  batch_size: 64        # Fits in ~12 GB GPU with mixed-precision
  training_mode: search
  patience: 30
  optimizer:
    type: rmsprop   # As they do in paper
    params:
      lr: 0.01
      alpha: 0.99
      momentum: 0.9
      weight_decay: 0.0001

  scheduler:
    type: cosine
    params:
      T_max: 300         # Matches the number of epochs

# --- Hyperparameter tuning ----------------------------------------
tuning:
  # Specify which cfg paths to tune and their [min, max] ranges
  param_grid:
    model.dropout_cls:    [0.1, 0.6]
    training.lr:          [0.0001, 0.2]

  # Stopping tolerance: stop when range width < epsilon * original_range
  relative_epsilon: 0.05

  # Depth of recursive narrowing (max number of refinements per param)
  max_depth: 5

  # How many candidates to sample each pass (e.g. [low, mid, high])
  num_candidates: 3

  # How many epochs to train per trial evaluation
  search_epochs: 30

  search_csv: search_results_nonstandard.csv  # Path to save the search results

  silent: false  # Set to true to suppress output during tuning