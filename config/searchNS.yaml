# --- Dataset ------------------------------------------------------
dataset:
  name: MAMe
  img_size: 256        # Keeps native resolution; stem reduces it to 64×64

# --- Model --------------------------------------------------------
model:
  type: InceptionNet
  training_mode: search2
  classifier:
    layers:     [2048, 1024]
    dropout:    0.5
    activation: ReLU


  stem_cfg:
    use_bn: true
    init_channels: 64
    kernel_size: 7
    pool_kernel_size: 2
  # ---------------------------------------------------------------
  # Inception stages ≈ GoogLeNet extended with SE + residual
  # ---------------------------------------------------------------
  stages:
    - blocks:
        - b0:        64
          b1:      [48,  64]
          b2:      [64,  96]
          pool_proj: 32
          use_bn:    true
          use_se:    true
          residual:  false
      downsample: true

    - blocks:
        - b0:        128
          b1:      [96, 128]
          b2:      [96, 128]
          pool_proj: 64
          use_bn:    true
          use_se:    true
          residual:  true
        - b0:        128
          b1:      [96, 128]
          b2:      [96, 128]
          pool_proj: 64
          use_bn:    true
          use_se:    true
          residual:  true
      downsample: true

    - blocks:
        - b0:        192
          b1:      [96, 160]
          b2:      [128,192]
          pool_proj: 96
          use_bn:    true
          use_se:    true
          residual:  true
        - b0:        192
          b1:      [96, 160]
          b2:      [128,192]
          pool_proj: 96
          use_bn:    true
          use_se:    true
          residual:  true
      downsample: false


augmentations:
  active: true
  resize: [256, 256]
  random_horizontal_flip: true
  random_rotation: 30
  random_resized_crop: [240, 240]
  scale: [0.8, 1.0]
  normalize:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

# --- Training -----------------------------------------------------
training:
  epochs: 300           # Long training for stabilization
  batch_size: 128      
  training_mode: search
  patience: 40
  optimizer:
    type: adamw   # As they do in paper
    params:
      lr: 0.003
      weight_decay: 0.02

  scheduler:
    type: cosine
    params:
      T_max: 300         # Matches the number of epochs
      eta_min: 0.0001    # Minimum learning rate